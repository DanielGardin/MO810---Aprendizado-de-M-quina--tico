{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicabilidade\n",
    "\n",
    "Este trabalho tem como objetivo o treinamento de modelos explicáveis para o problema de análise de crimes segundo características socio-econômicas nas regiões de São Paulo. Aqui, retornamos ao uso de todas as variáveis socio-econômicas para obtenção de explicações completas acerca da origem e desenvolvimento criminal nas regiões. No entanto, utilizaremos uma arquitetura de modelos diferente das utilizadas nos trabalhos anteriores.\n",
    "\n",
    "O desafio de prever crimes numa região poderia ser modelada como uma regressão para prever a distribuição dos boletins de ocorrência, por exemplo, no tempo. No entanto, regiões com baixo índice de criminalidade não possuem amostras de boletins de ocorrência capazes de se extrair uma distribuição que modele os BOs de forma satisfatória, ou até mesmo fiel. A forma que encontramos para minimizar este problema anteriormente foi de modelar o problema como um problema de classificação em regiões perigosas ou não, baseado apenas na quantidade de BOs registrados em dada região. No entanto, este tipo de redução carrega inúmeros vieses e não é justo para tomar qualquer tipo de decisão, principalmente devido a ambiguidade que um hard threshold causa ao diferenciar amostras cujo padrão é semelhante, mas o ground-truth induzido não é, por exemplo, classificar como perigoso regiões com mais de 1 BOs por dia em média classifica regiões com 0.99 BOs por dia como seguros.\n",
    "\n",
    "Pensando nisso e numa forma de se aprimorar o problema em si de forma que sua saída seja mais fácil de ser explicada, utilizamos um modelo híbrido de Regressão Inflada em zero. Esta abordagem melhora a capacidade de explicação ao separar regiões perigosas de não perigosas (zero e não-zero) e então obter um modelo de regressão para regiões perigosas, o que garante modelos capazes de quantificar a incidência criminal e não apenas sua existência.\n",
    "\n",
    "### Revisão Bibliográfica\n",
    "\n",
    "[Curiel et. al](https://link.springer.com/article/10.1007/s10940-017-9354-9) argumenta que a distribuição de ocorrências criminais ocorre segundo uma distribuição de Poisson-Binomial, onde cada indivíduo é selecionado por uma Binomial e então a ocorrência é modelada por uma distribuição de Poisson. Deste modo, escolhemos tratar nosso problema de forma que cada região é tratada como uma amostra de uma distribuição de Poisson que tenta prever a quantidade de BOs relatados naquela região. Uma regressão de Poisson utiliza um aproximador de esperança dado por\n",
    "\n",
    "$$\\mathbb{E}[y \\,|\\, x] = \\exp(\\theta^Tx)$$\n",
    "\n",
    "Que induz uma distribuição de probabilidade condicional de Poisson\n",
    "\n",
    "$$p_\\theta(y \\,|\\, x) = \\frac{\\exp(y \\cdot \\theta^Tx)}{y!} \\exp(-e^{\\theta^Tx})$$\n",
    "\n",
    "Os parâmetros $\\theta$ são então otimizados de forma a maximizar a verossimilhança dos dados $\\mathcal{D} = \\{x^{(i)}, y^{(i)}\\}_{i=1}^N$, \n",
    "\n",
    "$$\\theta = \\argmax_{\\theta}\\prod_{i=1}^N p_\\theta(y_i \\,|\\, x_i)$$\n",
    "\n",
    "Como trata-se de um problema de regressão em que "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme('notebook', 'whitegrid', 'Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danielgardin/.local/lib/python3.8/site-packages/sklearn/preprocessing/_function_transformer.py:340: UserWarning: With transform=\"pandas\", `func` should return a DataFrame to follow the set_output API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEN10</th>\n",
       "      <th>CMU10</th>\n",
       "      <th>ARISC</th>\n",
       "      <th>ESG10</th>\n",
       "      <th>DPP10</th>\n",
       "      <th>PMANC</th>\n",
       "      <th>CLUSTER</th>\n",
       "      <th>AGU10</th>\n",
       "      <th>VER10</th>\n",
       "      <th>PJM10</th>\n",
       "      <th>HOMIC</th>\n",
       "      <th>LIX10</th>\n",
       "      <th>CAR10</th>\n",
       "      <th>AGL10</th>\n",
       "      <th>POP10</th>\n",
       "      <th>CAL10</th>\n",
       "      <th>VIAGEM</th>\n",
       "      <th>EXURB</th>\n",
       "      <th>TDESL</th>\n",
       "      <th>DPI10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.3619</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.0558</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>0</td>\n",
       "      <td>806</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>1428.9</td>\n",
       "      <td>2</td>\n",
       "      <td>29.8</td>\n",
       "      <td>0.0075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.3268</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0701</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>0</td>\n",
       "      <td>913</td>\n",
       "      <td>0.9706</td>\n",
       "      <td>1554.2</td>\n",
       "      <td>1</td>\n",
       "      <td>21.7</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0560</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0</td>\n",
       "      <td>625</td>\n",
       "      <td>0.9788</td>\n",
       "      <td>1471.5</td>\n",
       "      <td>2</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.5414</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1271</td>\n",
       "      <td>0.0752</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0</td>\n",
       "      <td>572</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>1404.8</td>\n",
       "      <td>2</td>\n",
       "      <td>27.7</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.4458</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.0809</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>754</td>\n",
       "      <td>0.9667</td>\n",
       "      <td>1492.0</td>\n",
       "      <td>2</td>\n",
       "      <td>28.6</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DEN10   CMU10  ARISC  ESG10   DPP10  PMANC  CLUSTER  AGU10   VER10   PJM10  \\\n",
       "0      2  0.3619      0    1.0  0.9925      0      2.0    1.0  0.0410  0.0558   \n",
       "1      2  0.3268      0    1.0  1.0000      0      0.0    1.0  0.0000  0.0701   \n",
       "2      2  0.4392      0    1.0  1.0000      0      2.0    1.0  0.0370  0.0560   \n",
       "3      2  0.5414      0    1.0  1.0000      0      1.0    1.0  0.1271  0.0752   \n",
       "4      2  0.4458      0    1.0  1.0000      0      1.0    1.0  0.0500  0.0809   \n",
       "\n",
       "   HOMIC   LIX10   CAR10  AGL10  POP10   CAL10  VIAGEM  EXURB  TDESL   DPI10  \n",
       "0      1  0.9962  0.0224      0    806  0.9925  1428.9      2   29.8  0.0075  \n",
       "1      1  1.0000  0.0163      0    913  0.9706  1554.2      1   21.7  0.0000  \n",
       "2      1  1.0000  0.0106      0    625  0.9788  1471.5      2   25.8  0.0000  \n",
       "3      2  1.0000  0.0055      0    572  0.9834  1404.8      2   27.7  0.0000  \n",
       "4      2  1.0000  0.0000      0    754  0.9667  1492.0      2   28.6  0.0000  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../dataset/Padrão Urbano/PU.csv', sep=';')\n",
    "\n",
    "selected_columns = set(''.join([i for i in column if not i.isdigit()]) for column in data.columns)\n",
    "selected_columns = [f\"{column}10\"if len(column) == 3 else column for column in selected_columns]\n",
    "\n",
    "X = data[selected_columns].drop(columns=['SETTT'])\n",
    "\n",
    "categ_columns = ['HOMIC', 'ARISC', 'PMANC', 'EXURB', 'AGL10', 'DEN10', 'CLUSTER']\n",
    "log_columns = ['VIAGEM', 'POP10']\n",
    "\n",
    "X['CLUSTER'] = OrdinalEncoder().fit_transform(X[['CLUSTER']])\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"log_numeric\", make_pipeline(FunctionTransformer(np.log1p).set_output(transform=\"pandas\"), StandardScaler().set_output(transform=\"pandas\")), log_columns),\n",
    "    (\"onehot_categ\", OneHotEncoder(sparse_output=False).set_output(transform=\"pandas\"), categ_columns)\n",
    "], remainder = StandardScaler().set_output(transform=\"pandas\")).set_output(transform=\"pandas\")\n",
    "\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes = pd.read_csv('../dataset/Crimes/Listagem_Geral.csv', index_col=0)\n",
    "\n",
    "crimes[\"DATA\"] = pd.to_datetime(crimes[\"DATA\"])\n",
    "\n",
    "counts = crimes['SETOR'].value_counts()\n",
    "\n",
    "y = data['SETTT'].apply(lambda setor : counts.get(setor, 0)).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 0.6\n",
    "\n",
    "n_aug_samples = int(len(X_train) * frac)\n",
    "\n",
    "idx = np.random.randint(0, len(X_train), size=n_aug_samples)\n",
    "\n",
    "X_aug = X_train.iloc[idx].copy()\n",
    "\n",
    "X_aug[\"CAR10\"] = X_train[\"CAR10\"].sample(n=n_aug_samples, replace=True, random_state=21).to_numpy()\n",
    "\n",
    "y_aug = y_train[idx]\n",
    "\n",
    "X_aug = pd.concat([X_train, X_aug])\n",
    "y_aug = np.hstack([y_train, y_aug])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(models, X, y, metrics, cv=20, df=None, figname=None):\n",
    "    if df is None:\n",
    "        df = pd.DataFrame(columns=metrics.keys())\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        scores = {metric : [] for metric in metrics}\n",
    "\n",
    "        for train_idx, val_idx in tqdm(KFold(n_splits=cv).split(X, y)):\n",
    "            model.fit(X.iloc[train_idx], y[train_idx])\n",
    "\n",
    "            for metric_name, metric in metrics.items():\n",
    "                scores[metric_name].append(metric(model, X.iloc[val_idx], y[val_idx]))\n",
    "\n",
    "        results[name] = scores\n",
    "        df.loc[name, :] = scores\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('Algorithm Comparison')\n",
    "\n",
    "    n_metrics = len(metrics)\n",
    "    n_models  = len(models)\n",
    "\n",
    "    names = list(models.keys())\n",
    "\n",
    "    spacing = 0.1\n",
    "    delta   = (1 - spacing)*(1 - 1/n_metrics)/2\n",
    "\n",
    "    positions = np.hstack([np.linspace(i - delta, i + delta, n_metrics) for i in range(n_models)])\n",
    "    boxes = plt.boxplot(np.array([model_result[metric] for model_result in results.values() for metric in metrics]).T, positions=positions, widths=1.5*delta/(n_metrics-1), patch_artist=True)\n",
    "\n",
    "    ax.xaxis.set_major_formatter(lambda _, i : names[i//n_metrics] if i%n_metrics==1 else '')\n",
    "    \n",
    "    for i in range(n_metrics * n_models):\n",
    "        color = sns.color_palette(n_colors=n_metrics)[i%n_metrics]\n",
    "\n",
    "        boxes['boxes'][i].set_color((1, 1, 1, 0.4))\n",
    "        boxes['boxes'][i].set(facecolor=color, alpha=0.8)\n",
    "        boxes['medians'][i].set_color((0, 0, 0, 0.4))\n",
    "        boxes['whiskers'][2*i].set_color(color)\n",
    "        boxes['whiskers'][2*i+1].set_color(color)\n",
    "        boxes['caps'][2*i].set_color(color)\n",
    "        boxes['caps'][2*i+1].set_color(color)\n",
    "        boxes['fliers'][i].set_color(color)\n",
    "        boxes['fliers'][i].set_color(color)\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        boxes['boxes'][i].set_label(metric)\n",
    "\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend(frameon=True, bbox_to_anchor=(1.1, 1.05))\n",
    "    plt.ylim(-100, 1)\n",
    "\n",
    "    if figname is not None:\n",
    "        plt.savefig(f\"{figname}.png\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return df\n",
    "\n",
    "def style(df, format_table):\n",
    "\n",
    "    new_df = df.applymap(lambda x : f\"{np.mean(x)} $\\pm$ {np.std(x)}\")\n",
    "    return new_df.style.highlight_max().format(format_table,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import get_scorer, pairwise_distances, balanced_accuracy_score, get_scorer_names\n",
    "\n",
    "def consistency_score(estimator, X, y, k_neigh=5, metric=\"euclidean\"):\n",
    "    distances = pairwise_distances(X, metric=metric)\n",
    "\n",
    "    y_pred = estimator.predict(X)\n",
    "\n",
    "    y_neigh = y_pred[np.argsort(distances, axis=1)[:, 1:k_neigh+1]].mean(axis=1)\n",
    "\n",
    "    return 1 - np.abs(y - y_neigh).mean()\n",
    "\n",
    "def regression_accuracy(estimator, X, y, lim=1):\n",
    "    y_pred = estimator.predict(X)\n",
    "\n",
    "    y_pred = np.where(y_pred > lim, 1, 0)\n",
    "    y_norm = np.where(y > lim, 1, 0)\n",
    "\n",
    "    return balanced_accuracy_score(y_norm, y_pred)\n",
    "\n",
    "\n",
    "metrics = {\n",
    "    \"MAE\" : get_scorer(\"neg_mean_absolute_error\"),\n",
    "    \"Bal Acc\" : regression_accuracy,\n",
    "    \"Consistency\" : consistency_score\n",
    "}\n",
    "\n",
    "format_table = {\n",
    "    \"MAE\" : \"{:3f}\",\n",
    "    \"Bal Acc\" : \"{:.2%}\",\n",
    "    \"Consistency\" : \"{:3f}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_down(n):\n",
    "    if n == 0: return 0\n",
    "\n",
    "    base = 10**np.floor(np.log10(np.abs(n)))\n",
    "    return np.floor(n / base) * base\n",
    "\n",
    "def round_up(n):\n",
    "    if n == 0: return 0\n",
    "    \n",
    "    base = 10**np.floor(np.log10(np.abs(n)))\n",
    "    return np.ceil(n / base) * base\n",
    "\n",
    "def align_axes(main_ax:plt.Axes, twin_ax:plt.Axes, new_ticks=None):\n",
    "    l, u = main_ax.get_ylim()\n",
    "    ticks = main_ax.get_yticks()\n",
    "    ticks = ticks[(ticks >= l) & (ticks <= u)]\n",
    "\n",
    "    ticks_given = False\n",
    "    if new_ticks is None:\n",
    "        lower, upper = twin_ax.get_ylim()\n",
    "\n",
    "        max_delta = (upper - lower)/(len(ticks) - 1)\n",
    "\n",
    "        delta     = round_up(max_delta)\n",
    "\n",
    "        tick_min  = round_down(lower)\n",
    "\n",
    "        tick_max  = tick_min + delta * (len(ticks) - 1)\n",
    "\n",
    "    elif len(new_ticks) == 2:\n",
    "        tick_min, tick_max = new_ticks\n",
    "\n",
    "    elif len(new_ticks) == len(ticks): \n",
    "        tick_min, tick_max = new_ticks[0], new_ticks[-1]\n",
    "        ticks_given = True\n",
    "\n",
    "        if not isinstance(new_ticks, np.ndarray):\n",
    "            new_ticks = np.array(new_ticks)\n",
    "    else:\n",
    "        raise TypeError(\"Length of the target ticks do not match with the ticks on main axes.\")\n",
    "\n",
    "    bminusa = (tick_max - tick_min) * (u - l)/(ticks[-1] - ticks[0])\n",
    "\n",
    "    a = tick_max - (ticks[-1] - l)/(u - l) * bminusa\n",
    "    b = bminusa + a\n",
    "\n",
    "    if not ticks_given:\n",
    "        new_ticks = np.linspace(tick_min, tick_max, len(ticks))\n",
    "\n",
    "    twin_ax.set_yticks(new_ticks)\n",
    "    twin_ax.set_ylim(a, b)\n",
    "\n",
    "def plot_metrics(model, X, y, interval=(0, 100)):\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(16, 5), dpi=200)\n",
    "\n",
    "    liminf, limsup = interval\n",
    "\n",
    "    n ,bins, patches = ax[0].hist(y, bins=np.linspace(liminf, limsup, 100), alpha=.5, label='Ground truth')\n",
    "    y_pred = model.predict(X)\n",
    "    percent = np.count_nonzero((y_pred >= liminf) & (y_pred <= limsup))/len(y_pred)\n",
    "    \n",
    "    ax[0].hist(y_pred, bins=bins, alpha=.5, label=\"Predicted\")\n",
    "    ax[0].set_xlim(*interval)\n",
    "    ax[0].set_title(\"Distribution\")\n",
    "    ax[0].set_xlabel('Number of reports')\n",
    "    ax[0].set_ylabel('Count')\n",
    "    ax[0].legend()\n",
    "\n",
    "    twin_ax = ax[0].twinx()\n",
    "    cuts = np.arange(liminf, limsup)\n",
    "\n",
    "    bal_acc = [regression_accuracy(model, X, y, cut) for cut in cuts]\n",
    "\n",
    "    twin_ax.plot(cuts, bal_acc, label='Balanced accuracy')\n",
    "\n",
    "    align_axes(ax[0], twin_ax, (0, 1))\n",
    "\n",
    "    res = y - y_pred\n",
    "\n",
    "    liminf = min(np.quantile(res, percent), np.quantile(res, 1 - percent))\n",
    "    limsup = max(np.quantile(res, percent), np.quantile(res, 1 - percent))\n",
    "\n",
    "    ax[1].hist(res, bins=np.linspace(liminf, limsup, 100), alpha=.7)\n",
    "    ax[1].set_title(\"Residual distribution\")\n",
    "    ax[1].set_xlim(liminf, limsup)\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos interpretáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PoissonRegressor, QuantileRegressor, LinearRegression\n",
    "\n",
    "X_interp = preprocess.fit_transform(X_train)\n",
    "\n",
    "linear_model =  LinearRegression()\n",
    "\n",
    "poisson_model = PoissonRegressor(solver=\"newton-cholesky\", alpha=1e-4)\n",
    "\n",
    "quantile_model = QuantileRegressor(quantile=0.4, alpha=1e-5, solver='highs')\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"Linear Regressor\"   : linear_model,\n",
    "    \"Poisson Regressor\"  : poisson_model,\n",
    "    \"Quantile Regressor\" : quantile_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:16,  1.69s/it]\n",
      "4it [01:21, 20.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/danielgardin/Daniel/Acadêmico/Aulas/Mestrado/02 - 2S2023/MO810 - Aprendizado de Máquina Ético/T5 - Explicabilidade/Notebook 5.1 - Reformulação.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/danielgardin/Daniel/Acad%C3%AAmico/Aulas/Mestrado/02%20-%202S2023/MO810%20-%20Aprendizado%20de%20M%C3%A1quina%20%C3%89tico/T5%20-%20Explicabilidade/Notebook%205.1%20-%20Reformula%C3%A7%C3%A3o.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m results \u001b[39m=\u001b[39m evaluate(models, X_train, y_train, metrics, cv\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/media/danielgardin/Daniel/Acadêmico/Aulas/Mestrado/02 - 2S2023/MO810 - Aprendizado de Máquina Ético/T5 - Explicabilidade/Notebook 5.1 - Reformulação.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/danielgardin/Daniel/Acad%C3%AAmico/Aulas/Mestrado/02%20-%202S2023/MO810%20-%20Aprendizado%20de%20M%C3%A1quina%20%C3%89tico/T5%20-%20Explicabilidade/Notebook%205.1%20-%20Reformula%C3%A7%C3%A3o.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m scores \u001b[39m=\u001b[39m {metric : [] \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m metrics}\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/danielgardin/Daniel/Acad%C3%AAmico/Aulas/Mestrado/02%20-%202S2023/MO810%20-%20Aprendizado%20de%20M%C3%A1quina%20%C3%89tico/T5%20-%20Explicabilidade/Notebook%205.1%20-%20Reformula%C3%A7%C3%A3o.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m train_idx, val_idx \u001b[39min\u001b[39;00m tqdm(KFold(n_splits\u001b[39m=\u001b[39mcv)\u001b[39m.\u001b[39msplit(X, y)):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/danielgardin/Daniel/Acad%C3%AAmico/Aulas/Mestrado/02%20-%202S2023/MO810%20-%20Aprendizado%20de%20M%C3%A1quina%20%C3%89tico/T5%20-%20Explicabilidade/Notebook%205.1%20-%20Reformula%C3%A7%C3%A3o.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     model\u001b[39m.\u001b[39;49mfit(X\u001b[39m.\u001b[39;49miloc[train_idx], y[train_idx])\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/danielgardin/Daniel/Acad%C3%AAmico/Aulas/Mestrado/02%20-%202S2023/MO810%20-%20Aprendizado%20de%20M%C3%A1quina%20%C3%89tico/T5%20-%20Explicabilidade/Notebook%205.1%20-%20Reformula%C3%A7%C3%A3o.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mfor\u001b[39;00m metric_name, metric \u001b[39min\u001b[39;00m metrics\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/danielgardin/Daniel/Acad%C3%AAmico/Aulas/Mestrado/02%20-%202S2023/MO810%20-%20Aprendizado%20de%20M%C3%A1quina%20%C3%89tico/T5%20-%20Explicabilidade/Notebook%205.1%20-%20Reformula%C3%A7%C3%A3o.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         scores[metric_name]\u001b[39m.\u001b[39mappend(metric(model, X\u001b[39m.\u001b[39miloc[val_idx], y[val_idx]))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    404\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 405\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    407\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/linear_model/_quantile.py:280\u001b[0m, in \u001b[0;36mQuantileRegressor.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    276\u001b[0m         A_eq \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([X, \u001b[39m-\u001b[39mX, eye, \u001b[39m-\u001b[39meye], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    278\u001b[0m b_eq \u001b[39m=\u001b[39m y\n\u001b[0;32m--> 280\u001b[0m result \u001b[39m=\u001b[39m linprog(\n\u001b[1;32m    281\u001b[0m     c\u001b[39m=\u001b[39;49mc,\n\u001b[1;32m    282\u001b[0m     A_eq\u001b[39m=\u001b[39;49mA_eq,\n\u001b[1;32m    283\u001b[0m     b_eq\u001b[39m=\u001b[39;49mb_eq,\n\u001b[1;32m    284\u001b[0m     method\u001b[39m=\u001b[39;49msolver,\n\u001b[1;32m    285\u001b[0m     options\u001b[39m=\u001b[39;49msolver_options,\n\u001b[1;32m    286\u001b[0m )\n\u001b[1;32m    287\u001b[0m solution \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mx\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result\u001b[39m.\u001b[39msuccess:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_linprog.py:602\u001b[0m, in \u001b[0;36mlinprog\u001b[0;34m(c, A_ub, b_ub, A_eq, b_eq, bounds, method, callback, options, x0, integrality)\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mHiGHS solvers do not support the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    598\u001b[0m                               \u001b[39m\"\u001b[39m\u001b[39mcallback interface.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    599\u001b[0m highs_solvers \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mhighs-ipm\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mipm\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhighs-ds\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msimplex\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    600\u001b[0m                  \u001b[39m'\u001b[39m\u001b[39mhighs\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m}\n\u001b[0;32m--> 602\u001b[0m sol \u001b[39m=\u001b[39m _linprog_highs(lp, solver\u001b[39m=\u001b[39;49mhighs_solvers[meth],\n\u001b[1;32m    603\u001b[0m                      \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msolver_options)\n\u001b[1;32m    604\u001b[0m sol[\u001b[39m'\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m'\u001b[39m], sol[\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m (\n\u001b[1;32m    605\u001b[0m     _check_result(sol[\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m], sol[\u001b[39m'\u001b[39m\u001b[39mfun\u001b[39m\u001b[39m'\u001b[39m], sol[\u001b[39m'\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m'\u001b[39m], sol[\u001b[39m'\u001b[39m\u001b[39mslack\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    606\u001b[0m                   sol[\u001b[39m'\u001b[39m\u001b[39mcon\u001b[39m\u001b[39m'\u001b[39m], lp\u001b[39m.\u001b[39mbounds, tol, sol[\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m    607\u001b[0m sol[\u001b[39m'\u001b[39m\u001b[39msuccess\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m sol[\u001b[39m'\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_linprog_highs.py:363\u001b[0m, in \u001b[0;36m_linprog_highs\u001b[0;34m(lp, solver, time_limit, presolve, disp, maxiter, dual_feasibility_tolerance, primal_feasibility_tolerance, ipm_optimality_tolerance, simplex_dual_edge_weight_strategy, **unknown_options)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m     integrality \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(integrality)\n\u001b[0;32m--> 363\u001b[0m res \u001b[39m=\u001b[39m _highs_wrapper(c, A\u001b[39m.\u001b[39;49mindptr, A\u001b[39m.\u001b[39;49mindices, A\u001b[39m.\u001b[39;49mdata, lhs, rhs,\n\u001b[1;32m    364\u001b[0m                      lb, ub, integrality\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49muint8), options)\n\u001b[1;32m    366\u001b[0m \u001b[39m# HiGHS represents constraints as lhs/rhs, so\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m# Ax + s = b => Ax = b - s\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[39m# and we need to split up s by A_ub and A_eq\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mslack\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m res:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = evaluate(models, X_train, y_train, metrics, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>Bal Acc</th>\n",
       "      <th>Consistency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Poisson Regressor</th>\n",
       "      <td>-39.962 $\\pm$ 1.288</td>\n",
       "      <td>0.503 $\\pm$ 0.001</td>\n",
       "      <td>-54.626 $\\pm$ 2.378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantile Regressor</th>\n",
       "      <td>-4548.109 $\\pm$ 9014.024</td>\n",
       "      <td>0.642 $\\pm$ 0.020</td>\n",
       "      <td>-47.771 $\\pm$ 2.486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poisson GBTree</th>\n",
       "      <td>-37.585 $\\pm$ 2.075</td>\n",
       "      <td>0.567 $\\pm$ 0.016</td>\n",
       "      <td>-47.899 $\\pm$ 2.490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         MAE             Bal Acc  \\\n",
       "Poisson Regressor        -39.962 $\\pm$ 1.288   0.503 $\\pm$ 0.001   \n",
       "Quantile Regressor  -4548.109 $\\pm$ 9014.024   0.642 $\\pm$ 0.020   \n",
       "Poisson GBTree           -37.585 $\\pm$ 2.075   0.567 $\\pm$ 0.016   \n",
       "\n",
       "                            Consistency  \n",
       "Poisson Regressor   -54.626 $\\pm$ 2.378  \n",
       "Quantile Regressor  -47.771 $\\pm$ 2.486  \n",
       "Poisson GBTree      -47.899 $\\pm$ 2.490  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.applymap(lambda x : f\"{np.mean(x) : .3f} $\\pm$ {np.std(x) :.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_model = QuantileRegressor(quantile=0.1, alpha=1e-4, solver='highs').fit(X_interp, y_train)\n",
    "\n",
    "plot_metrics(quantile_model, X_interp, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Inflated Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (180656819.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 18\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"boost\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklego.meta import ZeroInflatedRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "zero_inflated_models = {\n",
    "    \"Poisson ZIR\" : Pipeline([\n",
    "        ('preprocess', preprocess),\n",
    "        ('model', ZeroInflatedRegressor(\n",
    "            LinearRegression(alpha=1e-6), PoissonRegressor(solver=\"newton-cholesky\", alpha=1e-6))\n",
    "        )\n",
    "        ]),\n",
    "    \"Quantile ZIR\" : Pipeline([\n",
    "        ('preprocess', preprocess),\n",
    "        ('model', ZeroInflatedRegressor(\n",
    "            LinearRegression(alpha=1e-6), PoissonRegressor(solver=\"newton-cholesky\", alpha=1e-6))\n",
    "        )\n",
    "        ])\n",
    "\n",
    "    \"boost\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos não-interpretáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "\n",
    "poisson_tree = HistGradientBoostingRegressor(loss=\"quantile\", max_leaf_nodes=128, quantile=0.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fairenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
