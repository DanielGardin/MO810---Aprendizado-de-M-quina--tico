{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicabilidade\n",
    "\n",
    "Este trabalho tem como objetivo o treinamento de modelos explicáveis para o problema de análise de crimes segundo características socio-econômicas nas regiões de São Paulo. Aqui, retornamos ao uso de todas as variáveis socio-econômicas para obtenção de explicações completas acerca da origem e desenvolvimento criminal nas regiões. No entanto, utilizaremos uma arquitetura de modelos diferente das utilizadas nos trabalhos anteriores.\n",
    "\n",
    "O desafio de prever crimes numa região poderia ser modelada como uma regressão para prever a distribuição dos boletins de ocorrência, por exemplo, no tempo. No entanto, regiões com baixo índice de criminalidade não possuem amostras de boletins de ocorrência capazes de se extrair uma distribuição que modele os BOs de forma satisfatória, ou até mesmo fiel. A forma que encontramos para minimizar este problema anteriormente foi de modelar o problema como um problema de classificação em regiões perigosas ou não, baseado apenas na quantidade de BOs registrados em dada região. No entanto, este tipo de redução carrega inúmeros vieses e não é justo para tomar qualquer tipo de decisão, principalmente devido a ambiguidade que um hard threshold causa ao diferenciar amostras cujo padrão é semelhante, mas o ground-truth induzido não é (por exemplo, classificar como perigoso regiões com mais de 1 BOs por dia em média classifica regiões com 0.99 BOs por dia como seguros.).\n",
    "\n",
    "Pensando nisso e numa forma de se aprimorar o problema em si de forma que sua saída seja mais fácil de ser explicada, utilizamos um modelo híbrido de Regressão Inflada em zero. Esta abordagem melhora a capacidade de explicação ao separar regiões perigosas de não perigosas (zero e não-zero) e então obter um modelo de regressão para regiões perigosas, o que garante modelos capazes de quantificar a incidência criminal e não apenas sua existência.\n",
    "\n",
    "### Revisão Bibliográfica\n",
    "\n",
    "[Curiel et. al](https://link.springer.com/article/10.1007/s10940-017-9354-9) argumenta que a distribuição de ocorrências criminais ocorre segundo uma distribuição de Poisson-Binomial, onde cada indivíduo é selecionado por uma Binomial e então a ocorrência é modelada por uma distribuição de Poisson. Deste modo, escolhemos tratar nosso problema de forma que cada região é tratada como uma amostra de uma distribuição de Poisson que tenta prever a quantidade de BOs relatados naquela região. Uma regressão de Poisson utiliza um aproximador de esperança dado por\n",
    "\n",
    "$$\\mathbb{E}[y \\,|\\, x] = \\exp(\\theta^Tx)$$\n",
    "\n",
    "Que induz uma distribuição de probabilidade condicional de Poisson\n",
    "\n",
    "$$p_\\theta(y \\,|\\, x) = \\frac{\\exp(y \\cdot \\theta^Tx)}{y!} \\exp(-e^{\\theta^Tx})$$\n",
    "\n",
    "Os parâmetros $\\theta$ são então otimizados de forma a maximizar a verossimilhança dos dados $\\mathcal{D} = \\{x^{(i)}, y^{(i)}\\}_{i=1}^N$, \n",
    "\n",
    "$$\\theta = \\argmax_{\\theta}\\prod_{i=1}^N p_\\theta(y_i \\,|\\, x_i)$$\n",
    "\n",
    "Como trata-se de um problema de regressão em que "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme('notebook', 'whitegrid', 'Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIAGEM</th>\n",
       "      <th>TDESL</th>\n",
       "      <th>HOMIC</th>\n",
       "      <th>ARISC</th>\n",
       "      <th>PMANC</th>\n",
       "      <th>EXURB</th>\n",
       "      <th>POP80</th>\n",
       "      <th>POP81</th>\n",
       "      <th>POP82</th>\n",
       "      <th>POP83</th>\n",
       "      <th>...</th>\n",
       "      <th>LIX10</th>\n",
       "      <th>PJM80</th>\n",
       "      <th>PJM91</th>\n",
       "      <th>PJM00</th>\n",
       "      <th>PJM10</th>\n",
       "      <th>VER80</th>\n",
       "      <th>VER91</th>\n",
       "      <th>VER00</th>\n",
       "      <th>VER10</th>\n",
       "      <th>CLUSTER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1428.9</td>\n",
       "      <td>29.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1050</td>\n",
       "      <td>1043</td>\n",
       "      <td>1035</td>\n",
       "      <td>1027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.0810</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.0658</td>\n",
       "      <td>0.0558</td>\n",
       "      <td>0.0407</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1554.2</td>\n",
       "      <td>21.7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1091</td>\n",
       "      <td>1079</td>\n",
       "      <td>1068</td>\n",
       "      <td>1057</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.1006</td>\n",
       "      <td>0.0802</td>\n",
       "      <td>0.0818</td>\n",
       "      <td>0.0701</td>\n",
       "      <td>0.4207</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1471.5</td>\n",
       "      <td>25.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>963</td>\n",
       "      <td>960</td>\n",
       "      <td>956</td>\n",
       "      <td>952</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0935</td>\n",
       "      <td>0.0863</td>\n",
       "      <td>0.0840</td>\n",
       "      <td>0.0560</td>\n",
       "      <td>0.0433</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0531</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1404.8</td>\n",
       "      <td>27.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1218</td>\n",
       "      <td>1183</td>\n",
       "      <td>1149</td>\n",
       "      <td>1115</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0904</td>\n",
       "      <td>0.0745</td>\n",
       "      <td>0.0658</td>\n",
       "      <td>0.0752</td>\n",
       "      <td>0.4054</td>\n",
       "      <td>0.1043</td>\n",
       "      <td>0.1602</td>\n",
       "      <td>0.1271</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1492.0</td>\n",
       "      <td>28.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1201</td>\n",
       "      <td>1181</td>\n",
       "      <td>1163</td>\n",
       "      <td>1146</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0863</td>\n",
       "      <td>0.1101</td>\n",
       "      <td>0.0809</td>\n",
       "      <td>0.5530</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0575</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   VIAGEM  TDESL  HOMIC  ARISC  PMANC  EXURB  POP80  POP81  POP82  POP83  ...  \\\n",
       "0  1428.9   29.8      1      0      0      2   1050   1043   1035   1027  ...   \n",
       "1  1554.2   21.7      1      0      0      1   1091   1079   1068   1057  ...   \n",
       "2  1471.5   25.8      1      0      0      2    963    960    956    952  ...   \n",
       "3  1404.8   27.7      2      0      0      2   1218   1183   1149   1115  ...   \n",
       "4  1492.0   28.6      2      0      0      2   1201   1181   1163   1146  ...   \n",
       "\n",
       "    LIX10   PJM80   PJM91   PJM00   PJM10   VER80   VER91   VER00   VER10  \\\n",
       "0  0.9962  0.0810  0.0759  0.0658  0.0558  0.0407  0.0215  0.0260  0.0410   \n",
       "1  1.0000  0.1006  0.0802  0.0818  0.0701  0.4207  0.0000  0.0068  0.0000   \n",
       "2  1.0000  0.0935  0.0863  0.0840  0.0560  0.0433  0.0000  0.0531  0.0370   \n",
       "3  1.0000  0.0904  0.0745  0.0658  0.0752  0.4054  0.1043  0.1602  0.1271   \n",
       "4  1.0000  0.0954  0.0863  0.1101  0.0809  0.5530  0.0000  0.0575  0.0500   \n",
       "\n",
       "   CLUSTER  \n",
       "0      2.0  \n",
       "1      0.0  \n",
       "2      2.0  \n",
       "3      1.0  \n",
       "4      1.0  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../dataset/Padrão Urbano/PU.csv', sep=';')\n",
    "\n",
    "X = data.drop(columns=['SETTT'])\n",
    "\n",
    "categ_columns = ['HOMIC', 'ARISC', 'PMANC', 'EXURB', 'AGL91', 'AGL00', 'AGL10', 'DEN80', 'DEN91', 'DEN00', 'DEN10', 'CLUSTER']\n",
    "log_columns = [feature for feature in X.columns if 'POP' in feature] + ['VIAGEM']\n",
    "\n",
    "X['CLUSTER'] = OrdinalEncoder().fit_transform(X[['CLUSTER']])\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"log_numeric\", make_pipeline(FunctionTransformer(np.log1p), StandardScaler()), log_columns),\n",
    "    (\"onehot_categ\", OneHotEncoder(), categ_columns)\n",
    "], remainder = StandardScaler())\n",
    "\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes = pd.read_csv('../dataset/Crimes/Listagem_Geral.csv', index_col=0)\n",
    "\n",
    "crimes[\"DATA\"] = pd.to_datetime(crimes[\"DATA\"])\n",
    "\n",
    "counts = crimes['SETOR'].value_counts()\n",
    "\n",
    "y = data['SETTT'].apply(lambda setor : counts.get(setor, 0)).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 0.6\n",
    "\n",
    "n_aug_samples = int(len(X_train) * frac)\n",
    "\n",
    "idx = np.random.randint(0, len(X_train), size=n_aug_samples)\n",
    "\n",
    "X_aug = X_train.iloc[idx].copy()\n",
    "\n",
    "X_aug[\"CAR10\"] = X_train[\"CAR10\"].sample(n=n_aug_samples, replace=True, random_state=21).to_numpy()\n",
    "\n",
    "y_aug = y_train[idx]\n",
    "\n",
    "X_aug = pd.concat([X_train, X_aug])\n",
    "y_aug = np.hstack([y_train, y_aug])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_labels(model, X, y, interval=(0, 100)):\n",
    "    fig, ax = plt.subplots(figsize=(8, 5), dpi=200)\n",
    "\n",
    "    n ,bins, patches = plt.hist(y, bins=np.linspace(interval[0], interval[1], 100), alpha=.5, label='Ground truth')\n",
    "    plt.hist(model.predict(X), bins=bins, alpha=.5, label=\"Predicted\")\n",
    "    plt.xlim(*interval)\n",
    "    plt.legend()\n",
    "    plt.title(\"Distribution \")\n",
    "    plt.xlabel('Number of reports')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def evaluate(models, X, y, X_test, y_test, metrics, cv=20, df=None, figname=None):\n",
    "    if df is None:\n",
    "        df = pd.DataFrame(columns=metrics.keys())\n",
    "    \n",
    "    results = {}\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        scores = {metric : [] for metric in metrics}\n",
    "\n",
    "        for train_idx, val_idx in KFold(n_splits=cv).split(X, y):\n",
    "            model.fit(X.iloc[train_idx], y[train_idx])\n",
    "\n",
    "            for metric_name, metric in metrics.items():\n",
    "                scores[metric_name].append(metric(model, X.iloc[val_idx], y[val_idx]))\n",
    "\n",
    "        results[name] = scores\n",
    "\n",
    "        model.fit(X, y)\n",
    "\n",
    "        result = {metric : scorer(model, X_test, y_test) for metric, scorer in metrics.items()}\n",
    "\n",
    "        df.loc[name, :] = result\n",
    "\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('Algorithm Comparison')\n",
    "\n",
    "    n_metrics = len(metrics)\n",
    "    n_models  = len(models)\n",
    "\n",
    "    names = list(models.keys())\n",
    "\n",
    "    spacing = 0.1\n",
    "    delta   = (1 - spacing)*(1 - 1/n_metrics)/2\n",
    "\n",
    "    positions = np.hstack([np.linspace(i - delta, i + delta, n_metrics) for i in range(n_models)])\n",
    "    boxes = plt.boxplot(np.array([model_result[metric] for model_result in results.values() for metric in metrics]).T, positions=positions, widths=1.5*delta/(n_metrics-1), patch_artist=True)\n",
    "\n",
    "    ax.xaxis.set_major_formatter(lambda _, i : names[i//n_metrics] if i%n_metrics==1 else '')\n",
    "\n",
    "    for i in range(n_metrics * n_models):\n",
    "        color = sns.color_palette(n_colors=n_metrics)[i%n_metrics]\n",
    "\n",
    "        boxes['boxes'][i].set_color((1, 1, 1, 0.4))\n",
    "        boxes['boxes'][i].set(facecolor=color, alpha=0.8)\n",
    "        boxes['medians'][i].set_color((0, 0, 0, 0.4))\n",
    "        boxes['whiskers'][2*i].set_color(color)\n",
    "        boxes['whiskers'][2*i+1].set_color(color)\n",
    "        boxes['caps'][2*i].set_color(color)\n",
    "        boxes['caps'][2*i+1].set_color(color)\n",
    "        boxes['fliers'][i].set_color(color)\n",
    "        boxes['fliers'][i].set_color(color)\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        boxes['boxes'][i].set_label(metric)\n",
    "\n",
    "    plt.ylim((0, 1))\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend(frameon=True, bbox_to_anchor=(1.1, 1.05))\n",
    "\n",
    "    if figname is not None:\n",
    "        plt.savefig(f\"{figname}.png\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return df\n",
    "\n",
    "def style(df, format_table):\n",
    "    return df.style.highlight_max().format(format_table,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Poisson Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import get_scorer, pairwise_distances\n",
    "\n",
    "def consistency_score(estimator, X, y, k_neigh=5, metric=\"euclidean\"):\n",
    "    distances = pairwise_distances(X, metric=metric)\n",
    "\n",
    "    y_pred = estimator.predict(X)\n",
    "\n",
    "    y_neigh = y_pred[np.argsort(distances, axis=1)[:, 1:k_neigh+1]].mean(axis=1)\n",
    "\n",
    "    return 1 - np.abs(y - y_neigh).mean()\n",
    "\n",
    "metrics = {\n",
    "    \"MAE\" : get_scorer(\"neg_mean_absolute_error\"),\n",
    "    \"MSE\" : get_scorer(\"neg_mean_squared_error\"),\n",
    "    \"Max Error\" : get_scorer(\"max_error\"),\n",
    "    \"Consistency\" : consistency_score\n",
    "}\n",
    "\n",
    "format_table = {\n",
    "    \"MAE\" : \"{:}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PoissonRegressor, QuantileRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "poisson_model = Pipeline([\n",
    "    ('preprocess', preprocess),\n",
    "    ('model', PoissonRegressor(solver=\"newton-cholesky\", alpha=1e-6))\n",
    "])\n",
    "\n",
    "quantile_model = QuantileRegressor(quantile=0.5, alpha=1e-5, solver='highs')\n",
    "\n",
    "poisson_tree = HistGradientBoostingRegressor(loss=\"quantile\", max_leaf_nodes=128, quantile=0.5)\n",
    "\n",
    "models = {\n",
    "    \"Poisson Regressor\"  : poisson_model,\n",
    "    \"Quantile Regressor\" : quantile_model,\n",
    "    \"Poisson GBTree\"     : poisson_tree\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danielgardin/.local/lib/python3.8/site-packages/sklearn/linear_model/_quantile.py:186: FutureWarning: The default solver will change from 'interior-point' to 'highs' in version 1.4. Set `solver='highs'` or to the desired solver to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "results = evaluate(models, X_aug, y_aug, X_test, y_test, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fairenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
